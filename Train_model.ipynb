{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем датасет, разобьем на трейн/тест. Отсортировав данные по таймстемпу, я решил выбрать отсечение на тест,после 40 процентов рейтингов. Такое решение принято из следующих соображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016396786841771056 0.1 1402\n",
      "0.07983296217149177 0.2 3113\n",
      "0.07543601933174869 0.3 3899\n",
      "0.07476501870912944 0.4 4211\n",
      "0.04923675499850952 0.5 4355\n",
      "0.04363576243210818 0.6 5140\n",
      "0.03541803450583221 0.7 5173\n",
      "0.030677347359815903 0.8 5534\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ratings.csv').sort_values(by='timestamp')\n",
    "train = 0.1\n",
    "\n",
    "for train in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "    sep_index = int(train * df.shape[0])\n",
    "\n",
    "    train_df = df[:sep_index]\n",
    "    test_df = df[sep_index:]\n",
    "\n",
    "    test_df = test_df.loc[test_df.userId.isin(train_df.userId)]\n",
    "    test_df = test_df.loc[test_df.movieId.isin(train_df.movieId)]\n",
    "    print(test_df.shape[0]/train_df.shape[0],train,test_df.userId.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу убрать из тестовой части людей, которых не было в трейне (так как на них все равно предсказаний не построишь). Также для простоты я хочу убрать фильмы из теста из тех, которых не было в трейне. Простота заключается в том, чтобы сформировать преобразование в уникальные номера порядковые только по трейну. Возможно, это можно сделать и без удаления лишних пользователей из теста, но не сейчас. В любом случае - отсутствие \"новых\" пользователей на тесте метрику не изменит. А отсутствие \"новых\" фильмов сместит ее в большую сторону. Однако, так как задание на сравнение моделей, то обе модели будут в одинаковых условиях. \n",
    "В общем, порог выбран на 0.4. Ниже создаются два файла: train_df.csv и test_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import Preprocessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "prep = Preprocessor('ratings.csv')\n",
    "prep.process(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "user_num = train_df.userId.max()\n",
    "movie_num = train_df.movieId.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим корпус для обучения w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_timestamp(x):\n",
    "    a = x.values\n",
    "    a.sort(1)\n",
    "    a = a[:,0]\n",
    "    return a.tolist()\n",
    "\n",
    "q = train_df.groupby(['userId'])[['movieId','timestamp']].apply(lambda x: sort_by_timestamp(x)).reset_index()\n",
    "q.rename({0:'sequences'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for elem in q.sequences.values:\n",
    "    corpus.append(list(map(str,elem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v params that configure w2v model for sngs\n",
    "w2v_params = { 'sg':1, # w2v is configured for skip-gram scheme\n",
    "              'negative':10,# negative sampling is set to 10\n",
    "              #in the article the authors mentioned size equals 100\n",
    "              #but they have vocabulary with ~1M. So I have\n",
    "              #approximately 12k movies thus I set 'size' to the 4th root\n",
    "              #and just in case I'll multiply it by 2\n",
    "              'size': 20,\n",
    "              'window':5, # according to the article\n",
    "              'min_count': 1 #nothing ignored\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39389761, 40000525)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song2vec = Word2Vec(corpus,**w2v_params)\n",
    "song2vec.train(corpus,total_examples=song2vec.corpus_count,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2vec.save('w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.x.iloc[idx]\n",
    "        \n",
    "        user = row.userId.astype(np.float32)\n",
    "        mov = row.movieId.astype(np.float32)\n",
    "        \n",
    "        most_similar = song2vec.wv.most_similar([str(int(mov))],topn=5)\n",
    "        most_similar_films = np.array([int(el[0]) for el in most_similar],dtype=np.float32)\n",
    "        most_similar_values = np.array([el[1] for el in most_similar],dtype=np.float32)\n",
    "\n",
    "        rating = row.rating.astype(np.float32) \n",
    "      \n",
    "        return user,mov,most_similar_films,most_similar_values,rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=64,\n",
    "                                           num_workers=2,shuffle=True)\n",
    "\n",
    "dloaders = {'train' : train_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MF(nn.Module):\n",
    "    \n",
    "    def __init__(self,user_num,movie_num,av_rating):\n",
    "        torch.manual_seed(0)\n",
    "        super(My_MF, self).__init__()\n",
    "        \n",
    "        self.av_rating = av_rating\n",
    "        \n",
    "        self.user_num = user_num\n",
    "        self.movie_num = movie_num       \n",
    "        \n",
    "        self.user_factors = nn.Embedding(int(user_num)+1,20)\n",
    "        self.movie_factors = nn.Embedding(int(movie_num)+1,20)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(int(user_num)+1,1)\n",
    "        self.movie_bias = nn.Embedding(int(movie_num)+1,1)\n",
    "        \n",
    "        self.soft = nn.Softmax(-1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, user, movie,ms_films,ms_values):\n",
    "        \n",
    "        user = user.long()\n",
    "        movie = movie.long()\n",
    "        ms_films = ms_films.long()\n",
    "        \n",
    "      #  print(user.shape, ' is a user.shape')\n",
    "      #  print(movie.shape, ' is a movie.shape')\n",
    "        movie_embedded = self.movie_factors(movie)\n",
    "        user_embedded = self.user_factors(user)\n",
    "      #  print(movie_embedded.shape, ' is a movie_embded.shape')\n",
    "      #  print(user_embedded.shape, ' is a user_embeded.shape')\n",
    "        \n",
    "        dot_product = (user_embedded*movie_embedded).sum(1)\n",
    "        \n",
    "        user_bias = self.user_bias(user).squeeze(1)\n",
    "        movie_bias = self.movie_bias(movie).squeeze(1)\n",
    "      #  print(user_bias.shape, ' is a shape of user_bias')\n",
    "      #  print(movie_bias.shape, ' is a shape of movie_bias')\n",
    "      #  print(dot_product.shape, ' is a shape of dot_product')\n",
    "\n",
    "        ratings = self.av_rating + dot_product + user_bias + movie_bias\n",
    "        \n",
    "        \n",
    "\n",
    "        reg_part = (movie_embedded**2).sum(1)+(user_embedded**2).sum(1)+user_bias**2+movie_bias**2\n",
    "       # print(reg_part.shape,' is a shape of reg_part')\n",
    "        \n",
    "        ms_embedded = self.movie_factors(ms_films)\n",
    "        #print(ms_embedded.shape,' is a shape of sim_embedded')\n",
    "        movie_repeated = movie_embedded.unsqueeze(1).repeat(1,5,1)\n",
    "       # print(movie_repeated.shape, ' is a shape of movie_repeated')\n",
    "        \n",
    "        dot_product = (ms_embedded *movie_repeated).sum(2)\n",
    "        #print(dot_product.shape, ' is a shape of dot_product')\n",
    "        \n",
    "        sgns_together = ((ms_values - dot_product)**2).sum(1)\n",
    "        \n",
    "        #print(sgns_together.shape, 'is a shape of sgns_together')\n",
    "        \n",
    "        \n",
    "        return ratings,reg_part,sgns_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be1a6af46ae411b93d55691667fb498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.321903781890867 22.145557\n",
      "26.380586166381836 20.24787\n",
      "23.79698763847351 18.861406\n",
      "23.19866027832031 19.200268\n",
      "21.07975595474243 17.716072\n",
      "19.66129549026489 16.740644\n",
      "18.591427087783813 16.033436\n",
      "17.115189924240113 14.869233\n",
      "16.420250396728516 14.387356\n",
      "15.244388666152954 13.449464\n",
      "15.256924953460693 13.583665\n",
      "14.379351539611816 12.839761\n",
      "13.726345376968384 12.275247\n",
      "13.124957876205444 11.764063\n",
      "12.708343029022217 11.419276\n",
      "11.315589451789856 10.112691\n",
      "11.201719431877136 10.065304\n",
      "10.853782806396485 9.761142\n",
      "11.03688859462738 9.971822\n",
      "10.18271809577942 9.187729\n",
      "9.676022925376891 8.691547\n",
      "9.480864353179932 8.510547\n",
      "9.452612113952636 8.512612\n",
      "8.755849409103394 7.8678784\n",
      "8.567514624595642 7.6712694\n",
      "8.402208499908447 7.5396047\n",
      "7.844388837814331 7.0005493\n",
      "7.7207704496383665 6.888556\n",
      "7.372390398979187 6.5492344\n",
      "7.20496129989624 6.401637\n",
      "7.1000201654434205 6.315993\n",
      "6.842363634109497 6.0688906\n",
      "6.411538627147674 5.653588\n",
      "6.48086416721344 5.719796\n",
      "6.102865960597992 5.358896\n",
      "6.222124934196472 5.4730415\n",
      "5.687372386455536 4.95083\n",
      "5.6221830487251285 4.893722\n",
      "5.543629713058472 4.831288\n",
      "5.362442381381989 4.6427855\n",
      "5.601783938407898 4.896212\n",
      "5.185050575733185 4.4820137\n",
      "4.8216495704650875 4.134631\n",
      "4.736963016986847 4.051526\n",
      "4.913070530891418 4.227456\n",
      "4.649098343849182 3.978668\n",
      "4.701559991836548 4.019953\n",
      "4.435455615520477 3.7721143\n",
      "4.541047441959381 3.8756714\n",
      "4.251616699695587 3.5942557\n",
      "4.039277138710022 3.3853278\n",
      "4.14031261920929 3.4885018\n"
     ]
    }
   ],
   "source": [
    "mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2))\n",
    "#mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2)).to(device)\n",
    "#mdl.load_state_dict(torch.load('models/my_als2.0.pkl',map_location='cpu'))\n",
    "mdl = mdl.to(device)\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer = optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "\n",
    "if not os.path.exists('models/'):\n",
    "    os.makedir('models')\n",
    "\n",
    "lam = 0.025\n",
    "alpha = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    rem = []\n",
    "    true_loss = []\n",
    "    for i,(user,movie,ms_films,ms_values,ratings) in tqdm_notebook(enumerate(dloaders['train'])):\n",
    "        \n",
    "        user = user.to(device)\n",
    "        movie = movie.to(device)\n",
    "        ms_films = ms_films.to(device)\n",
    "        ms_values = ms_values.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        ratings_pred,reg_part,sgns = mdl(user,movie,ms_films,ms_values)\n",
    "\n",
    "        loss += (((ratings.float() - ratings_pred)**2) +lam*reg_part+alpha*sgns).mean()\n",
    "        \n",
    "        true_loss.append(((ratings.float() - ratings_pred)**2).cpu().detach().numpy())\n",
    "        \n",
    "\n",
    "        if i % 1 == 0 and i > 3:\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rem.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            \n",
    "        if i % 500 == 499:\n",
    "            print(np.mean(rem[-100:]),np.mean(true_loss[-100:]))\n",
    "            torch.save(mdl.state_dict(), 'models/my_als{}.pkl'.format(round(np.mean(rem[-100:]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
