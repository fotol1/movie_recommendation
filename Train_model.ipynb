{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем датасет, разобьем на трейн/тест. Отсортировав данные по таймстемпу, я решил выбрать отсечение на тест,после 40 процентов рейтингов. Такое решение принято из следующих соображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016396786841771056 0.1 1402\n",
      "0.07983296217149177 0.2 3113\n",
      "0.07543601933174869 0.3 3899\n",
      "0.07476501870912944 0.4 4211\n",
      "0.04923675499850952 0.5 4355\n",
      "0.04363576243210818 0.6 5140\n",
      "0.03541803450583221 0.7 5173\n",
      "0.030677347359815903 0.8 5534\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ratings.csv').sort_values(by='timestamp')\n",
    "train = 0.1\n",
    "\n",
    "for train in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "    sep_index = int(train * df.shape[0])\n",
    "\n",
    "    train_df = df[:sep_index]\n",
    "    test_df = df[sep_index:]\n",
    "\n",
    "    test_df = test_df.loc[test_df.userId.isin(train_df.userId)]\n",
    "    test_df = test_df.loc[test_df.movieId.isin(train_df.movieId)]\n",
    "    print(test_df.shape[0]/train_df.shape[0],train,test_df.userId.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу убрать из тестовой части людей, которых не было в трейне (так как на них все равно предсказаний не построишь). Также для простоты я хочу убрать фильмы из теста из тех, которых не было в трейне. Простота заключается в том, чтобы сформировать преобразование в уникальные номера порядковые только по трейну. Возможно, это можно сделать и без удаления лишних пользователей из теста, но не сейчас. В любом случае - отсутствие \"новых\" пользователей на тесте метрику не изменит. А отсутствие \"новых\" фильмов сместит ее в большую сторону. Однако, так как задание на сравнение моделей, то обе модели будут в одинаковых условиях. \n",
    "В общем, порог выбран на 0.4. Ниже создаются два файла: train_df.csv и test_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import Preprocessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "prep = Preprocessor('ratings.csv')\n",
    "prep.process(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "user_num = train_df.userId.max()\n",
    "movie_num = train_df.movieId.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим корпус для обучения w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_timestamp(x):\n",
    "    a = x.values\n",
    "    a.sort(1)\n",
    "    a = a[:,0]\n",
    "    return a.tolist()\n",
    "\n",
    "q = train_df.groupby(['userId'])[['movieId','timestamp']].apply(lambda x: sort_by_timestamp(x)).reset_index()\n",
    "q.rename({0:'sequences'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for elem in q.sequences.values:\n",
    "    corpus.append(list(map(str,elem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v params that configure w2v model for sngs\n",
    "w2v_params = { 'sg':1, # w2v is configured for skip-gram scheme\n",
    "              'negative':10,# negative sampling is set to 10\n",
    "              #in the article the authors mentioned size equals 100\n",
    "              #but they have vocabulary with ~1M. So I have\n",
    "              #approximately 12k movies thus I set 'size' to the 4th root\n",
    "              #and just in case I'll multiply it by 2\n",
    "              'size': 20,\n",
    "              'window':5, # according to the article\n",
    "              'min_count': 1 #nothing ignored\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39389761, 40000525)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song2vec = Word2Vec(corpus,**w2v_params)\n",
    "song2vec.train(corpus,total_examples=song2vec.corpus_count,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2vec.save('w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_df.groupby('movieId').agg({'rating':'mean'}).reset_index().rename({'rating':'movie_bias'},axis=1)\n",
    "train_df = train_df.merge(temp)\n",
    "\n",
    "temp = train_df.groupby('userId').agg({'rating':'mean'}).reset_index().rename({'rating':'user_bias'},axis=1)\n",
    "train_df = train_df.merge(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.x.iloc[idx]\n",
    "        \n",
    "        user = row.userId.astype(np.float32)\n",
    "        mov = row.movieId.astype(np.float32)\n",
    "        \n",
    "        most_similar = song2vec.wv.most_similar([str(int(mov))],topn=5)\n",
    "        most_similar_films = np.array([int(el[0]) for el in most_similar],dtype=np.float32)\n",
    "        most_similar_values = np.array([el[1] for el in most_similar],dtype=np.float32)\n",
    "\n",
    "        rating = row.rating.astype(np.float32) \n",
    "      \n",
    "        return user,mov,most_similar_films,most_similar_values,rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['rating'] = train_df['rating']/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=32,\n",
    "                                           num_workers=2,shuffle=True)\n",
    "\n",
    "dloaders = {'train' : train_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (a) in enumerate(dloaders['train']):\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ScaledEmbedding(nn.Embedding):\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
    "        if self.padding_idx is not None:\n",
    "            self.weight.data[self.padding_idx].fill_(0)\n",
    "\n",
    "\n",
    "class ZeroEmbedding(nn.Embedding):\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weight.data.zero_()\n",
    "        if self.padding_idx is not None:\n",
    "            self.weight.data[self.padding_idx].fill_(0)\n",
    "\n",
    "class My_MF(nn.Module):\n",
    "    \n",
    "    def __init__(self,user_num,movie_num,av_rating):\n",
    "        torch.manual_seed(0)\n",
    "        super(My_MF, self).__init__()\n",
    "        \n",
    "        self.av_rating = av_rating\n",
    "        \n",
    "        self.user_num = user_num\n",
    "        self.movie_num = movie_num       \n",
    "        \n",
    "        self.user_factors = ScaledEmbedding(int(user_num)+1,20,sparse=True)\n",
    "        self.movie_factors = ScaledEmbedding(int(movie_num)+1,20,sparse=True)\n",
    "        \n",
    "        self.user_bias = ZeroEmbedding(int(user_num)+1,1,sparse=True)\n",
    "        self.movie_bias = ZeroEmbedding(int(movie_num)+1,1,sparse=True)\n",
    "        \n",
    "        self.soft = nn.Softmax(-1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, user, movie,ms_films,ms_values):\n",
    "        \n",
    "        user = user.long()\n",
    "        movie = movie.long()\n",
    "        ms_films = ms_films.long()\n",
    "        \n",
    "      #  print(user.shape, ' is a user.shape')\n",
    "      #  print(movie.shape, ' is a movie.shape')\n",
    "        movie_embedded = self.movie_factors(movie)\n",
    "        user_embedded = self.user_factors(user)\n",
    "      #  print(movie_embedded.shape, ' is a movie_embded.shape')\n",
    "      #  print(user_embedded.shape, ' is a user_embeded.shape')\n",
    "        \n",
    "        dot_product = (user_embedded*movie_embedded).sum(1)\n",
    "        \n",
    "        user_bias = self.user_bias(user).squeeze(1)\n",
    "        movie_bias = self.movie_bias(movie).squeeze(1)\n",
    "        \n",
    "      #  print(user_bias.shape, ' is a shape of user_bias')\n",
    "      #  print(movie_bias.shape, ' is a shape of movie_bias')\n",
    "      #  print(dot_product.shape, ' is a shape of dot_product')\n",
    "\n",
    "        ratings = self.av_rating + dot_product + user_bias + movie_bias\n",
    "       # ratings = dot_product\n",
    "\n",
    "        reg_part = (movie_embedded**2).sum(1)+(user_embedded**2).sum(1)+user_bias**2+movie_bias**2\n",
    "       # print(reg_part.shape,' is a shape of reg_part')\n",
    "        \n",
    "        ms_embedded = self.movie_factors(ms_films)\n",
    "        #print(ms_embedded.shape,' is a shape of sim_embedded')\n",
    "        movie_repeated = movie_embedded.unsqueeze(1).repeat(1,5,1)\n",
    "       # print(movie_repeated.shape, ' is a shape of movie_repeated')\n",
    "        \n",
    "        dot_product = (ms_embedded *movie_repeated).sum(2)\n",
    "        #print(dot_product.shape, ' is a shape of dot_product')\n",
    "        \n",
    "        sgns_together = ((ms_values - dot_product)**2).sum(1)\n",
    "        \n",
    "        #print(sgns_together.shape, 'is a shape of sgns_together')\n",
    "        \n",
    "        \n",
    "        return ratings,reg_part,sgns_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de0e83040d5420aabbf7c612ae9491a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0078780603408815 1.2233194\n",
      "5.632321047782898 1.2162623\n",
      "5.43015193939209 1.1780273\n",
      "5.326886878013611 1.1720891\n",
      "5.1975532579422 1.1830232\n",
      "5.101087851524353 1.1735008\n",
      "5.063956966400147 1.1794643\n",
      "5.0022532176971435 1.172167\n",
      "4.877777037620544 1.1376096\n",
      "4.861561851501465 1.1717389\n",
      "4.8224587678909305 1.1611745\n",
      "4.780946526527405 1.1628081\n",
      "4.749094362258911 1.1840705\n",
      "4.7205848884582515 1.1767399\n",
      "4.590779843330384 1.0995597\n",
      "4.685019702911377 1.1982205\n",
      "4.654230170249939 1.2180464\n",
      "4.613546123504639 1.1973151\n",
      "4.576393833160401 1.170075\n",
      "4.551136474609375 1.1761082\n",
      "4.496640188694 1.1358207\n",
      "4.468989446163177 1.130836\n",
      "4.4622173547744755 1.1374586\n",
      "4.488854339122772 1.1630564\n",
      "4.433083262443542 1.1472188\n",
      "4.403402330875397 1.1242102\n",
      "4.430470337867737 1.1547557\n",
      "4.445835030078888 1.1816989\n",
      "4.398363168239594 1.1360958\n",
      "4.374366307258606 1.1400815\n",
      "4.360946927070618 1.1330296\n",
      "4.372446479797364 1.1774167\n",
      "4.313180193901062 1.1293118\n",
      "4.4086977696418765 1.2245021\n",
      "4.35903279542923 1.183913\n",
      "4.329931607246399 1.1514724\n",
      "4.277194881439209 1.1248304\n",
      "4.322800667285919 1.1620983\n",
      "4.335293297767639 1.1718796\n",
      "4.270499939918518 1.1178606\n",
      "4.307921931743622 1.1834242\n",
      "4.2361156940460205 1.1094416\n",
      "4.257697334289551 1.1426568\n",
      "4.235059132575989 1.1418538\n",
      "4.257292354106903 1.1617\n",
      "4.2537028074264525 1.1536016\n",
      "4.233874273300171 1.1609178\n",
      "4.269265487194061 1.1709261\n",
      "4.215261654853821 1.1404783\n",
      "4.208734333515167 1.124048\n",
      "4.201668577194214 1.1374646\n",
      "4.223160936832428 1.1764653\n",
      "4.194008600711823 1.1437005\n",
      "4.188199644088745 1.1403064\n"
     ]
    }
   ],
   "source": [
    "mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2))\n",
    "#mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2)).to(device)\n",
    "#mdl.load_state_dict(torch.load('models/my_als2.0.pkl',map_location='cpu'))\n",
    "mdl = mdl.to(device)\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer = optim.SparseAdam(mdl.parameters(), lr=1e-3)\n",
    "#optimizer = optim.SGD(mdl.parameters(), lr=1e-2,weight_decay=1)\n",
    "\n",
    "if not os.path.exists('models/'):\n",
    "    os.makedir('models')\n",
    "\n",
    "lam = 30\n",
    "alpha = 0.7\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    rem = []\n",
    "    true_loss = []\n",
    "    for i,(user,movie,ms_films,ms_values,ratings) in tqdm_notebook(enumerate(dloaders['train'])):\n",
    "        \n",
    "        user = user.to(device)\n",
    "        movie = movie.to(device)\n",
    "        ms_films = ms_films.to(device)\n",
    "        ms_values = ms_values.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        ratings_pred,reg_part,sgns= mdl(user,movie,ms_films,ms_values)\n",
    "\n",
    "        loss += loss_func(ratings,ratings_pred)\\\n",
    "               +(lam*reg_part).mean()\\\n",
    "            +(alpha*sgns).mean()\n",
    "        \n",
    "        true_loss.append(((ratings.float() - ratings_pred)**2).cpu().detach().numpy())\n",
    "      #  print(loss_func(ratings,ratings_pred).item(),(lam*reg_part).mean().item(),(alpha*sgns).mean().item())\n",
    "\n",
    "        if i % 1 == 0 and i > 3:\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rem.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            \n",
    "        if i % 500 == 499:\n",
    "            print(np.mean(rem[-100:]),np.mean(true_loss[-100:]))\n",
    "            torch.save(mdl.state_dict(), 'models/my_als{}.pkl'.format(round(np.mean(rem[-100:]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
