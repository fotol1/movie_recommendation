{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем датасет, разобьем на трейн/тест. Отсортировав данные по таймстемпу, я решил выбрать отсечение на тест,после 40 процентов рейтингов. Такое решение принято из следующих соображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016396786841771056 0.1 1402\n",
      "0.07983296217149177 0.2 3113\n",
      "0.07543601933174869 0.3 3899\n",
      "0.07476501870912944 0.4 4211\n",
      "0.04923675499850952 0.5 4355\n",
      "0.04363576243210818 0.6 5140\n",
      "0.03541803450583221 0.7 5173\n",
      "0.030677347359815903 0.8 5534\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ratings.csv').sort_values(by='timestamp')\n",
    "train = 0.1\n",
    "\n",
    "for train in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "    sep_index = int(train * df.shape[0])\n",
    "\n",
    "    train_df = df[:sep_index]\n",
    "    test_df = df[sep_index:]\n",
    "\n",
    "    test_df = test_df.loc[test_df.userId.isin(train_df.userId)]\n",
    "    test_df = test_df.loc[test_df.movieId.isin(train_df.movieId)]\n",
    "    print(test_df.shape[0]/train_df.shape[0],train,test_df.userId.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу убрать из тестовой части людей, которых не было в трейне (так как на них все равно предсказаний не построишь). Также для простоты я хочу убрать фильмы из теста из тех, которых не было в трейне. Простота заключается в том, чтобы сформировать преобразование в уникальные номера порядковые только по трейну. Возможно, это можно сделать и без удаления лишних пользователей из теста, но не сейчас. В любом случае - отсутствие \"новых\" пользователей на тесте метрику не изменит. А отсутствие \"новых\" фильмов сместит ее в большую сторону. Однако, так как задание на сравнение моделей, то обе модели будут в одинаковых условиях. \n",
    "В общем, порог выбран на 0.4. Ниже создаются два файла: train_df.csv и test_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import Preprocessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "prep = Preprocessor('ratings.csv')\n",
    "prep.process(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "user_num = train_df.userId.max()\n",
    "movie_num = train_df.movieId.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим корпус для обучения w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_timestamp(x):\n",
    "    a = x.values\n",
    "    a.sort(1)\n",
    "    a = a[:,0]\n",
    "    return a.tolist()\n",
    "\n",
    "q = train_df.groupby(['userId'])[['movieId','timestamp']].apply(lambda x: sort_by_timestamp(x)).reset_index()\n",
    "q.rename({0:'sequences'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for elem in q.sequences.values:\n",
    "    corpus.append(list(map(str,elem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v params that configure w2v model for sngs\n",
    "w2v_params = { 'sg':1, # w2v is configured for skip-gram scheme\n",
    "              'negative':10,# negative sampling is set to 10\n",
    "              #in the article the authors mentioned size equals 100\n",
    "              #but they have vocabulary with ~1M. So I have\n",
    "              #approximately 12k movies thus I set 'size' to the 4th root\n",
    "              #and just in case I'll multiply it by 2\n",
    "              'size': 20,\n",
    "              'window':5, # according to the article\n",
    "              'min_count': 1 #nothing ignored\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39389408, 40000525)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song2vec = Word2Vec(corpus,**w2v_params)\n",
    "song2vec.train(corpus,total_examples=song2vec.corpus_count,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2vec.save('w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.x.iloc[idx]\n",
    "        \n",
    "        user = row.userId.astype(np.float32)\n",
    "        mov = row.movieId.astype(np.float32)\n",
    "        \n",
    "        most_similar = song2vec.wv.most_similar([str(int(mov))],topn=5)\n",
    "        most_similar_films = np.array([int(el[0]) for el in most_similar],dtype=np.float32)\n",
    "        most_similar_values = np.array([el[1] for el in most_similar],dtype=np.float32)\n",
    "\n",
    "        rating = row.rating.astype(np.float32) \n",
    "      \n",
    "        return user,mov,most_similar_films,most_similar_values,rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=64,\n",
    "                                           num_workers=2,shuffle=True)\n",
    "\n",
    "dloaders = {'train' : train_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MF(nn.Module):\n",
    "    \n",
    "    def __init__(self,user_num,movie_num,av_rating):\n",
    "        torch.manual_seed(0)\n",
    "        super(My_MF, self).__init__()\n",
    "        \n",
    "        self.av_rating = av_rating\n",
    "        \n",
    "        self.user_num = user_num\n",
    "        self.movie_num = movie_num       \n",
    "        \n",
    "        self.user_factors = nn.Embedding(int(user_num)+1,20)\n",
    "        self.movie_factors = nn.Embedding(int(movie_num)+1,20)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(int(user_num)+1,1)\n",
    "        self.movie_bias = nn.Embedding(int(movie_num)+1,1)\n",
    "        \n",
    "        self.soft = nn.Softmax(-1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, user, movie,ms_films,ms_values):\n",
    "        \n",
    "        user = user.long()\n",
    "        movie = movie.long()\n",
    "        ms_films = ms_films.long()\n",
    "        \n",
    "      #  print(user.shape, ' is a user.shape')\n",
    "      #  print(movie.shape, ' is a movie.shape')\n",
    "        movie_embedded = self.movie_factors(movie)\n",
    "        user_embedded = self.user_factors(user)\n",
    "      #  print(movie_embedded.shape, ' is a movie_embded.shape')\n",
    "      #  print(user_embedded.shape, ' is a user_embeded.shape')\n",
    "        \n",
    "        dot_product = (user_embedded*movie_embedded).sum(1)\n",
    "        \n",
    "        user_bias = self.user_bias(user).squeeze(1)\n",
    "        movie_bias = self.movie_bias(movie).squeeze(1)\n",
    "      #  print(user_bias.shape, ' is a shape of user_bias')\n",
    "      #  print(movie_bias.shape, ' is a shape of movie_bias')\n",
    "      #  print(dot_product.shape, ' is a shape of dot_product')\n",
    "\n",
    "        ratings = self.av_rating + dot_product + user_bias + movie_bias\n",
    "        \n",
    "        \n",
    "\n",
    "        reg_part = (movie_embedded**2).sum(1)+(user_embedded**2).sum(1)+user_bias**2+movie_bias**2\n",
    "       # print(reg_part.shape,' is a shape of reg_part')\n",
    "        \n",
    "        ms_embedded = self.movie_factors(ms_films)\n",
    "        #print(ms_embedded.shape,' is a shape of sim_embedded')\n",
    "        movie_repeated = movie_embedded.unsqueeze(1).repeat(1,5,1)\n",
    "       # print(movie_repeated.shape, ' is a shape of movie_repeated')\n",
    "        \n",
    "        dot_product = (ms_embedded *movie_repeated).sum(2)\n",
    "        #print(dot_product.shape, ' is a shape of dot_product')\n",
    "        \n",
    "        sgns_together = ((ms_values - dot_product)**2).sum(1)\n",
    "        \n",
    "        #print(sgns_together.shape, 'is a shape of sgns_together')\n",
    "        \n",
    "        \n",
    "        return ratings,reg_part#,sgns_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e47b000e15944a59facc4fbbf2ff9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-56b73bdea732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mrem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2))\n",
    "#mdl = My_MF(user_num,movie_num,round(train_df.rating.mean(),2)).to(device)\n",
    "#mdl.load_state_dict(torch.load('models/my_als2.0.pkl',map_location='cpu'))\n",
    "mdl = mdl.to(device)\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer = optim.Adam(mdl.parameters(), lr=1e-3)\n",
    "\n",
    "if not os.path.exists('models/'):\n",
    "    os.makedir('models')\n",
    "\n",
    "lam = 0.025\n",
    "alpha = 0.1\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    rem = []\n",
    "    true_loss = []\n",
    "    for i,(user,movie,ms_films,ms_values,ratings) in tqdm_notebook(enumerate(dloaders['train'])):\n",
    "        \n",
    "        user = user.to(device)\n",
    "        movie = movie.to(device)\n",
    "        ms_films = ms_films.to(device)\n",
    "        ms_values = ms_values.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        ratings_pred,reg_part = mdl(user,movie,ms_films,ms_values)\n",
    "\n",
    "        loss += (((ratings.float() - ratings_pred)**2) +lam*reg_part).mean()\n",
    "                # +\\\n",
    "               # alpha*sgns\n",
    "                #).mean()\n",
    "        \n",
    "        true_loss.append(((ratings.float() - ratings_pred)**2).cpu().detach().numpy())\n",
    "        \n",
    "\n",
    "        if i % 1 == 0 and i > 3:\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rem.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            \n",
    "        if i % 500 == 499:\n",
    "            print(np.mean(rem[-100:]),np.mean(true_loss[-100:]))\n",
    "            torch.save(mdl.state_dict(), 'models/my_als{}.pkl'.format(round(np.mean(rem[-100:]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71693, 6049)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_num,movie_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
